{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import a4_utils as a4u\n",
    "import UZ_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [8, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: What kind of structures in the image are detected by the algorithm?\n",
    "How does the parameter sigma affect the result?\n",
    "\n",
    "It determines the size of corners that are to be detected. A large corner looked at from up close is only an edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussdx(sigma: float):\n",
    "    side_width = int(math.ceil(3*sigma))\n",
    "    kernel_size = int(2 * side_width +1)\n",
    "    kernel_x_values = np.array(range(-side_width, side_width+1))\n",
    "    coef = -1 / ((sigma ** 3) * math.sqrt(2*math.pi))\n",
    "    kernel =  coef * kernel_x_values * np.exp( -(kernel_x_values ** 2) / (2* (sigma**2)) )\n",
    "    \n",
    "    kernel_abs = np.abs(kernel)\n",
    "    kernel /= np.sum(kernel_abs)\n",
    "    return kernel, kernel_x_values\n",
    "\n",
    "def gauss(sigma: float):\n",
    "    side_width = int(math.ceil(3*sigma))\n",
    "    kernel_size = int(2 * side_width +1)\n",
    "    kernel_x_values = np.array(range(-side_width, side_width+1))\n",
    "    coef = 1 / (sigma * math.sqrt(2*math.pi))\n",
    "    kernel =  coef * np.exp( -(kernel_x_values ** 2) / (2* (sigma**2)) )\n",
    "    kernel /= np.sum(kernel)\n",
    "    return kernel, kernel_x_values\n",
    "\n",
    "def gauss_kern_horizontal_faux2D(sigma: float):\n",
    "    ker = gauss(sigma)[0].reshape(1, -1)\n",
    "    return ker\n",
    "\n",
    "def gaussdx_kern_horizontal_faux2D(sigma: float):\n",
    "    ker = gaussdx(sigma)[0].reshape(1, -1)\n",
    "    return ker\n",
    "\n",
    "def image_partial_derivatives(image, sigma: float):\n",
    "    x_gauss = gauss_kern_horizontal_faux2D(sigma)\n",
    "    y_gauss = x_gauss.T\n",
    "    x_gauss_der = gaussdx_kern_horizontal_faux2D(sigma)\n",
    "    y_gauss_der = x_gauss_der.T\n",
    "\n",
    "    x_partial = UZ_utils.convolve(image, y_gauss, x_gauss_der)\n",
    "    y_partial = UZ_utils.convolve(image, x_gauss, y_gauss_der)\n",
    "    return x_partial, y_partial\n",
    "\n",
    "def image_second_partial_ders(image, sigma: float):\n",
    "    x_partial, y_partial = image_partial_derivatives(image, sigma)\n",
    "    \n",
    "    x_second_partial, xy_second_partial = image_partial_derivatives(x_partial, sigma)\n",
    "    _, y_second_partial = image_partial_derivatives(y_partial, sigma)\n",
    "\n",
    "    return x_second_partial, xy_second_partial, y_second_partial\n",
    "\n",
    "def hessian_feature_points(image, sigma, t, neigh_one_side_size):\n",
    "    \"\"\"\n",
    "    dx_kernel = -np.ones((1, 2))\n",
    "    dx_kernel[0,0] = 1\n",
    "    print(\"dx_kernel\")\n",
    "    print(dx_kernel)\n",
    "\n",
    "    dy_kernel = -np.ones((2, 1))\n",
    "    dy_kernel[0,0] = 1\n",
    "    print(\"dy_kernel\")\n",
    "    print(dy_kernel)\n",
    "\n",
    "\n",
    "    gauss_1dim = a4u.gauss(sigma)\n",
    "    gauss_2dim = gauss_1dim.T * gauss_1dim\n",
    "    print(\"gauss_2dim\")\n",
    "    print(gauss_2dim)\n",
    "    print(\"gauss_2dim.sum()\")\n",
    "    print(gauss_2dim.sum())\n",
    "    UZ_utils.imshow(gauss_2dim)\n",
    "\n",
    "\n",
    "    graf_gauss = a4u.convolve(image, gauss_2dim)\n",
    "\n",
    "    graf_gauss = graf_gauss / graf_gauss.max()\n",
    "\n",
    "    graf_dx = a4u.convolve(graf_gauss, dx_kernel)\n",
    "    graf_dy = a4u.convolve(graf_gauss, dy_kernel)\n",
    "\n",
    "    graf_xx = a4u.convolve(graf_dx, dx_kernel)\n",
    "    graf_xy = a4u.convolve(graf_dx, dy_kernel)\n",
    "    graf_yy = a4u.convolve(graf_dy, dy_kernel)\n",
    "    \"\"\"\n",
    "\n",
    "    graf_xx, graf_xy, graf_yy = image_second_partial_ders(image, sigma)\n",
    "    graf_hessian_det = graf_xx * graf_yy - graf_xy * graf_xy\n",
    "    # print(\"graf_hessian_det.shape\")\n",
    "    # print(graf_hessian_det.shape)\n",
    "    # print(\"graf_hessian_det.max()\")\n",
    "    # print(graf_hessian_det.max())\n",
    "    # print(\"graf_hessian_det\")\n",
    "    # print(graf_hessian_det)\n",
    "    graf_feature_points = graf_hessian_det * (graf_hessian_det > t)\n",
    "    # print(\"image\")\n",
    "    # print(graf_feature_points)\n",
    "\n",
    "    return graf_feature_points\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def nonmaxima_surpress(image, neigh_one_side_size):\n",
    "    image_non_maxima = np.zeros(image.shape)\n",
    "\n",
    "    for y_ix in range(image.shape[0]):\n",
    "        for x_ix in range(image.shape[1]):\n",
    "            y_bot = y_ix - neigh_one_side_size\n",
    "            y_top = y_ix + neigh_one_side_size\n",
    "            x_left = x_ix - neigh_one_side_size\n",
    "            x_right = x_ix + neigh_one_side_size\n",
    "\n",
    "            if y_bot < 0:\n",
    "                y_bot = 0\n",
    "            if y_top > image.shape[0] - 1:\n",
    "                y_top = image.shape[0] - 1\n",
    "            \n",
    "            if x_left < 0:\n",
    "                x_left = 0\n",
    "            if x_right > image.shape[1] - 1:\n",
    "                x_right = image.shape[1] - 1\n",
    "\n",
    "            neigh_max = image[y_bot:y_top, x_left:x_right].max()\n",
    "            if image[y_ix, x_ix] == neigh_max:\n",
    "                image_non_maxima[y_ix, x_ix] = image[y_ix, x_ix]\n",
    "            \n",
    "\n",
    "    return image_non_maxima\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "g_sigmas = [3, 6, 9]\n",
    "g_t = 0.004\n",
    "g_neigh_one_side = 7\n",
    "\n",
    "\n",
    "graf_a = UZ_utils.imread_gray(\".\\data\\graf\\graf_a.jpg\")\n",
    "\n",
    "\n",
    "for i, curr_sigma in enumerate(g_sigmas):\n",
    "    # print(\"i, curr_sigma\")\n",
    "    # print(i, curr_sigma)\n",
    "\n",
    "\n",
    "    # print(\"graf_a.shape\")\n",
    "    # print(graf_a.shape)\n",
    "    feature_points = hessian_feature_points(graf_a, curr_sigma, g_t, g_neigh_one_side)\n",
    "\n",
    "    # print(feature_points)\n",
    "    plt.subplot(2, 3, 1 + i)\n",
    "    plt.imshow(feature_points)\n",
    "    \n",
    "    fp_non_maxima = nonmaxima_surpress(feature_points, g_neigh_one_side)\n",
    "    fp_image_ix_pairs = np.nonzero(fp_non_maxima)\n",
    "\n",
    "    plt.subplot(2, 3, 4 + i)\n",
    "    plt.imshow(graf_a, cmap=\"gray\")\n",
    "    plt.scatter(fp_image_ix_pairs[1], fp_image_ix_pairs[0], marker=\"x\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def harris_feature_points(image, sigma, smoothing_sigma, alpha, tresh, neigh_one_side_size):\n",
    "    x_partial, y_partial = image_partial_derivatives(image, sigma)\n",
    "\n",
    "    x_partial_squared = x_partial * x_partial\n",
    "    x_partial_times_y_partial = x_partial * y_partial\n",
    "    y_partial_squared = y_partial * y_partial\n",
    "\n",
    "    gauss_1dim = a4u.gauss(smoothing_sigma)\n",
    "    gauss_2dim = gauss_1dim.T * gauss_1dim\n",
    "    # print(\"gauss_2dim\")\n",
    "    # print(gauss_2dim)\n",
    "    # print(\"gauss_2dim.sum()\")\n",
    "    # print(gauss_2dim.sum())\n",
    "    # UZ_utils.imshow(gauss_2dim)\n",
    "\n",
    "    x_partial_squared_smoothed = a4u.convolve(x_partial_squared, gauss_2dim)\n",
    "    x_partial_times_y_partial_smoothed = a4u.convolve(x_partial_times_y_partial, gauss_2dim)\n",
    "    y_partial_squared_smoothed = a4u.convolve(y_partial_squared, gauss_2dim)\n",
    "\n",
    "    determinant = x_partial_squared_smoothed * y_partial_squared_smoothed - x_partial_times_y_partial_smoothed * x_partial_times_y_partial_smoothed\n",
    "    trace_squared = (x_partial_squared_smoothed + y_partial_squared_smoothed) * (x_partial_squared_smoothed + y_partial_squared_smoothed)\n",
    "\n",
    "    combined_result = determinant - alpha * trace_squared\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    graf_feature_points = combined_result * (combined_result > tresh)\n",
    "\n",
    "    return graf_feature_points\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "g_sigmas = [3, 6, 9]\n",
    "g_smoothing_sigma = [1.6 * i for i in g_sigmas]\n",
    "g_t = 1e-4\n",
    "g_alpha = 0.06\n",
    "g_neigh_one_side = 5\n",
    "\n",
    "\n",
    "graf_a = UZ_utils.imread_gray(\".\\data\\graf\\graf_a.jpg\")\n",
    "\n",
    "\n",
    "for i, curr_sigma in enumerate(g_sigmas):\n",
    "    # print(\"i, curr_sigma\")\n",
    "    # print(i, curr_sigma)\n",
    "\n",
    "\n",
    "    # print(\"graf_a.shape\")\n",
    "    # print(graf_a.shape)\n",
    "    feature_points = harris_feature_points(graf_a, curr_sigma, g_smoothing_sigma[i], g_alpha, g_t, g_neigh_one_side)\n",
    "\n",
    "    fp_non_maxima = nonmaxima_surpress(feature_points, g_neigh_one_side)\n",
    "    fp_image_ix_pairs = np.nonzero(fp_non_maxima)\n",
    "\n",
    "\n",
    "    # print(feature_points)\n",
    "    plt.subplot(2, 3, 1 + i)\n",
    "    plt.imshow(feature_points)\n",
    "    \n",
    "    fp_non_maxima = nonmaxima_surpress(feature_points, g_neigh_one_side)\n",
    "    fp_image_ix_pairs = np.nonzero(fp_non_maxima)\n",
    "\n",
    "    plt.subplot(2, 3, 4 + i)\n",
    "    plt.imshow(graf_a, cmap=\"gray\")\n",
    "    plt.scatter(fp_image_ix_pairs[1], fp_image_ix_pairs[0], marker=\"x\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hellinger_distance(hist1, hist2):\n",
    "    hist_root_diff = hist1 ** (1/2) - hist2 ** (1/2)\n",
    "    hist_dist = (1/2 * np.sum(hist_root_diff ** 2)) ** (1/2)\n",
    "    return hist_dist\n",
    "\n",
    "def find_correspondances(features_a, features_b):\n",
    "    \"\"\"\n",
    "    features_a is a list of lists.\n",
    "    Its 0th element is a list of descriptors for the 0th feature point.\n",
    "\n",
    "    Returns 2 arrays.\n",
    "    The 0th elem of the first array contains the ix from features_b, that was\n",
    "    the best corresponding point to the 0th element of features_a.\n",
    "\n",
    "    The second returning array is vice versa.\n",
    "    \"\"\"\n",
    "    features_a_best_matches = np.zeros(features_a.shape[0])\n",
    "    features_b_best_matches = np.zeros(features_b.shape[0])\n",
    "\n",
    "    for a_ix in range(features_a.shape[0]):\n",
    "        b_ix_current_best = -1\n",
    "        dist_current_best = float('inf')\n",
    "\n",
    "        for b_ix in range(features_b.shape[0]):\n",
    "            dist_current = hellinger_distance(features_a[a_ix], features_b[b_ix])\n",
    "            if dist_current < dist_current_best:\n",
    "                dist_current_best = dist_current\n",
    "                b_ix_current_best = b_ix\n",
    "        \n",
    "        features_a_best_matches[a_ix] = b_ix_current_best\n",
    "    \n",
    "\n",
    "    for b_ix in range(features_b.shape[0]):\n",
    "        a_ix_current_best = -1\n",
    "        dist_current_best = float('inf')\n",
    "\n",
    "        for a_ix in range(features_a.shape[0]):\n",
    "            dist_current = hellinger_distance(features_b[b_ix], features_a[a_ix])\n",
    "            if dist_current < dist_current_best:\n",
    "                dist_current_best = dist_current\n",
    "                a_ix_current_best = a_ix\n",
    "        \n",
    "        features_b_best_matches[b_ix] = a_ix_current_best\n",
    "\n",
    "    \n",
    "    features_a_best_matches = features_a_best_matches.astype('int')\n",
    "    features_b_best_matches = features_b_best_matches.astype('int')\n",
    "\n",
    "    return features_a_best_matches, features_b_best_matches\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "g_sigma = 3\n",
    "g_t_hessian = 0.004\n",
    "g_non_max_neigh = 3\n",
    "\n",
    "g_t_harris = 1e-6\n",
    "g_smoothing_sigma = 1.6 * g_sigma\n",
    "g_alpha = 0.06\n",
    "\n",
    "g_n_bins = 16\n",
    "\n",
    "graf_a = UZ_utils.imread_gray(\".\\data\\graf\\graf_a_small.jpg\")\n",
    "graf_b = UZ_utils.imread_gray(\".\\data\\graf\\graf_b_small.jpg\")\n",
    "\n",
    "\n",
    "def simple_desc_harris(image, sigma, smoothing_sigma, alpha, tresh, non_max_neigh, n_bins):\n",
    "    \"\"\"\n",
    "    Returns: fp_image_ix_pairs, simple_descs\n",
    "    \"\"\"\n",
    "    \n",
    "    fp_image = harris_feature_points(image, sigma, smoothing_sigma, alpha,  tresh, non_max_neigh)\n",
    "    \n",
    "    fp_non_maxima = nonmaxima_surpress(fp_image, non_max_neigh)\n",
    "    fp_image_ix_pairs = np.nonzero(fp_non_maxima)\n",
    "\n",
    "    simple_descs = a4u.simple_descriptors(image, fp_image_ix_pairs[0], fp_image_ix_pairs[1], n_bins)\n",
    "    # print(simple_descriptors)\n",
    "\n",
    "    return fp_image_ix_pairs, simple_descs\n",
    "\n",
    "\n",
    "def simple_desc_hessian(image, sigma, tresh, non_max_neigh, n_bins):\n",
    "    \"\"\"\n",
    "    Returns: fp_image_ix_pairs, simple_descs\n",
    "    \"\"\"\n",
    "    \n",
    "    fp_image = hessian_feature_points(image, sigma, tresh, non_max_neigh)\n",
    "\n",
    "    fp_non_maxima = nonmaxima_surpress(fp_image, non_max_neigh)\n",
    "    fp_image_ix_pairs = np.nonzero(fp_non_maxima)\n",
    "\n",
    "    simple_descs = a4u.simple_descriptors(image, fp_image_ix_pairs[0], fp_image_ix_pairs[1], n_bins)\n",
    "    # print(simple_descriptors)\n",
    "\n",
    "    return fp_image_ix_pairs, simple_descs\n",
    "\n",
    "\n",
    "\n",
    "fp_image_ixs_a, simple_descs_a = simple_desc_hessian(graf_a, g_sigma, g_t_hessian, g_non_max_neigh, g_n_bins)\n",
    "fp_image_ixs_b, simple_descs_b = simple_desc_hessian(graf_b, g_sigma, g_t_hessian, g_non_max_neigh, g_n_bins)\n",
    "\n",
    "# these are the indexes of the other best matches, They point to their most corresponding point.\n",
    "fp_a_best_matches, fp_b_best_matches = find_correspondances(simple_descs_a, simple_descs_b)\n",
    "\n",
    "# print(\"fp_a_best_matches\")\n",
    "# print(fp_a_best_matches)\n",
    "\n",
    "# print(\"fp_image_ixs_b\")\n",
    "# print(fp_image_ixs_b)\n",
    "\n",
    "# Now we make Nx2 vectors, left column being the x coordinate. The a4u function asks for this.\n",
    "fp_a_match_image_ixs = np.zeros((fp_a_best_matches.size, 2))\n",
    "for i in range(fp_a_best_matches.size):\n",
    "    ix = fp_a_best_matches[i]\n",
    "\n",
    "    fp_a_match_image_ixs[i, 0] = fp_image_ixs_b[1][ix]\n",
    "    fp_a_match_image_ixs[i, 1] = fp_image_ixs_b[0][ix]\n",
    "\n",
    "\n",
    "fp_image_ixs_a_for_display = np.zeros((fp_image_ixs_a[0].size, 2))\n",
    "for i in range(fp_image_ixs_a[0].size):\n",
    "    fp_image_ixs_a_for_display[i, 0] = fp_image_ixs_a[1][i]\n",
    "    fp_image_ixs_a_for_display[i, 1] = fp_image_ixs_a[0][i]\n",
    "\n",
    "\n",
    "a4u.display_matches(graf_a, fp_image_ixs_a_for_display, graf_b, fp_a_match_image_ixs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing it for harris also:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_image_ixs_a, simple_descs_a = simple_desc_harris(graf_a, g_sigma, g_smoothing_sigma, g_alpha, g_t_harris, g_non_max_neigh, g_n_bins)\n",
    "fp_image_ixs_b, simple_descs_b = simple_desc_harris(graf_b, g_sigma, g_smoothing_sigma, g_alpha, g_t_harris, g_non_max_neigh, g_n_bins)\n",
    "\n",
    "# these are the indexes of the other best matches, They point to their most corresponding point.\n",
    "fp_a_best_matches, fp_b_best_matches = find_correspondances(simple_descs_a, simple_descs_b)\n",
    "\n",
    "# print(\"fp_a_best_matches\")\n",
    "# print(fp_a_best_matches)\n",
    "\n",
    "# print(\"fp_image_ixs_b\")\n",
    "# print(fp_image_ixs_b)\n",
    "\n",
    "# Now we make Nx2 vectors, left column being the x coordinate. The a4u function asks for this.\n",
    "fp_a_match_image_ixs = np.zeros((fp_a_best_matches.size, 2))\n",
    "for i in range(fp_a_best_matches.size):\n",
    "    ix = fp_a_best_matches[i]\n",
    "\n",
    "    fp_a_match_image_ixs[i, 0] = fp_image_ixs_b[1][ix]\n",
    "    fp_a_match_image_ixs[i, 1] = fp_image_ixs_b[0][ix]\n",
    "\n",
    "\n",
    "fp_image_ixs_a_for_display = np.zeros((fp_image_ixs_a[0].size, 2))\n",
    "for i in range(fp_image_ixs_a[0].size):\n",
    "    fp_image_ixs_a_for_display[i, 0] = fp_image_ixs_a[1][i]\n",
    "    fp_image_ixs_a_for_display[i, 1] = fp_image_ixs_a[0][i]\n",
    "\n",
    "\n",
    "a4u.display_matches(graf_a, fp_image_ixs_a_for_display, graf_b, fp_a_match_image_ixs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: What do you notice when visualizing the correspondences? How accurate are the matches?\n",
    "Hessian seems to have a higher percentage of inliers. Harris works okay but matches some points that shouldn't be matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_matches_hessian(image_a, image_b):\n",
    "    sigma = 3\n",
    "    t = 0.004\n",
    "    \n",
    "    non_max_neigh = 7\n",
    "\n",
    "\n",
    "    n_bins = 16\n",
    "\n",
    "    fp_image_ixs_a, simple_descs_a = simple_desc_hessian(image_a, sigma, t, non_max_neigh, n_bins)\n",
    "    fp_image_ixs_b, simple_descs_b = simple_desc_hessian(image_b, sigma, t, non_max_neigh, n_bins)\n",
    "\n",
    "    fp_image_ixs_a = list(zip(fp_image_ixs_a[0], fp_image_ixs_a[1]))\n",
    "    fp_image_ixs_b = list(zip(fp_image_ixs_b[0], fp_image_ixs_b[1]))\n",
    "\n",
    "    # these are the indexes of the other best matches, They point to their most corresponding point.\n",
    "    fp_a_best_matches, fp_b_best_matches = find_correspondances(simple_descs_a, simple_descs_b)\n",
    "\n",
    "    fp_symetric_matches = []\n",
    "\n",
    "    for a_ix in range(fp_a_best_matches.size):\n",
    "        matched_b_ix = fp_a_best_matches[a_ix]\n",
    "        matches_match = fp_b_best_matches[matched_b_ix]\n",
    "\n",
    "        if a_ix == matches_match:\n",
    "            fp_symetric_matches.append((fp_image_ixs_a[a_ix], fp_image_ixs_b[matched_b_ix]))\n",
    "\n",
    "    return fp_symetric_matches\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "graf_a = UZ_utils.imread_gray(\".\\data\\graf\\graf_a_small.jpg\")\n",
    "graf_b = UZ_utils.imread_gray(\".\\data\\graf\\graf_b_small.jpg\")\n",
    "\n",
    "matched_pairs_of_ixs = find_matches_hessian(graf_a, graf_b)\n",
    "\n",
    "graf_a_array_for_display = np.zeros((len(matched_pairs_of_ixs), 2))\n",
    "graf_b_array_for_display = np.zeros((len(matched_pairs_of_ixs), 2))\n",
    "\n",
    "for i in range(len(matched_pairs_of_ixs)):\n",
    "    graf_a_array_for_display[i, 0] = matched_pairs_of_ixs[i][0][1]\n",
    "    graf_a_array_for_display[i, 1] = matched_pairs_of_ixs[i][0][0]\n",
    "    graf_b_array_for_display[i, 0] = matched_pairs_of_ixs[i][1][1]\n",
    "    graf_b_array_for_display[i, 1] = matched_pairs_of_ixs[i][1][0]\n",
    "\n",
    "\n",
    "a4u.display_matches(graf_a, graf_a_array_for_display, graf_b, graf_b_array_for_display)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches_harris(image_a, image_b):\n",
    "    sigma = 3\n",
    "    smoothing_sigma = 1.6 * sigma\n",
    "    t = 1e-6\n",
    "    \n",
    "    \n",
    "    alpha = 0.06\n",
    "    non_max_neigh = 7\n",
    "\n",
    "\n",
    "    n_bins = 16\n",
    "\n",
    "    fp_image_ixs_a, simple_descs_a = simple_desc_harris(image_a, sigma, smoothing_sigma, alpha, t, non_max_neigh, n_bins)\n",
    "    fp_image_ixs_b, simple_descs_b = simple_desc_harris(image_b, sigma, smoothing_sigma, alpha, t, non_max_neigh, n_bins)\n",
    "\n",
    "    fp_image_ixs_a = list(zip(fp_image_ixs_a[0], fp_image_ixs_a[1]))\n",
    "    fp_image_ixs_b = list(zip(fp_image_ixs_b[0], fp_image_ixs_b[1]))\n",
    "\n",
    "    # these are the indexes of the other best matches, They point to their most corresponding point.\n",
    "    fp_a_best_matches, fp_b_best_matches = find_correspondances(simple_descs_a, simple_descs_b)\n",
    "\n",
    "    fp_symetric_matches = []\n",
    "\n",
    "    for a_ix in range(fp_a_best_matches.size):\n",
    "        matched_b_ix = fp_a_best_matches[a_ix]\n",
    "        matches_match = fp_b_best_matches[matched_b_ix]\n",
    "\n",
    "        if a_ix == matches_match:\n",
    "            fp_symetric_matches.append((fp_image_ixs_a[a_ix], fp_image_ixs_b[matched_b_ix]))\n",
    "\n",
    "    return fp_symetric_matches\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "graf_a = UZ_utils.imread_gray(\".\\data\\graf\\graf_a_small.jpg\")\n",
    "graf_b = UZ_utils.imread_gray(\".\\data\\graf\\graf_b_small.jpg\")\n",
    "\n",
    "matched_pairs_of_ixs = find_matches_harris(graf_a, graf_b)\n",
    "\n",
    "graf_a_array_for_display = np.zeros((len(matched_pairs_of_ixs), 2))\n",
    "graf_b_array_for_display = np.zeros((len(matched_pairs_of_ixs), 2))\n",
    "\n",
    "for i in range(len(matched_pairs_of_ixs)):\n",
    "    graf_a_array_for_display[i, 0] = matched_pairs_of_ixs[i][0][1]\n",
    "    graf_a_array_for_display[i, 1] = matched_pairs_of_ixs[i][0][0]\n",
    "    graf_b_array_for_display[i, 0] = matched_pairs_of_ixs[i][1][1]\n",
    "    graf_b_array_for_display[i, 1] = matched_pairs_of_ixs[i][1][0]\n",
    "\n",
    "\n",
    "a4u.display_matches(graf_a, graf_a_array_for_display, graf_b, graf_b_array_for_display)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Looking at the equation above, which parameters account for translation and which for rotation and scale?\n",
    "p3 and p4 for translation, p1 and p2 for scaling and rotation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estimate_homography(correspondences):\n",
    "\n",
    "    xr = correspondences[:,0]\n",
    "    yr = correspondences[:,1]\n",
    "    xt = correspondences[:,2]\n",
    "    yt = correspondences[:,3]\n",
    "\n",
    "\n",
    "    A = np.zeros((correspondences.shape[0] * 2, 9))\n",
    "\n",
    "    for i in range(correspondences.shape[0]):\n",
    "        ix = i * 2\n",
    "        A[ix, 0] = xr[i]\n",
    "        A[ix, 1] = yr[i]\n",
    "        A[ix, 2] = 1\n",
    "        A[ix, 6] = -xt[i]* xr[i]\n",
    "        A[ix, 7] = -xt[i]* yr[i]\n",
    "        A[ix, 8] = -xt[i]\n",
    "    \n",
    "    for i in range(correspondences.shape[0]):\n",
    "        ix = i * 2 + 1\n",
    "        A[ix, 3] = xr[i]\n",
    "        A[ix, 4] = yr[i]\n",
    "        A[ix, 5] = 1\n",
    "        A[ix, 6] = -yt[i]* xr[i]\n",
    "        A[ix, 7] = -yt[i]* yr[i]\n",
    "        A[ix, 8] = -yt[i]\n",
    "    \n",
    "\n",
    "    U, S, VT = np.linalg.svd(A)\n",
    "    V = VT.T\n",
    "\n",
    "    h = V[:, -1] / V[-1, -1]\n",
    "\n",
    "    H = h.reshape(3,3)\n",
    "    # print(H)\n",
    "\n",
    "    return H\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "I_a = UZ_utils.imread_gray(\".\\data\\\\newyork\\\\newyork_a.jpg\")\n",
    "I_b = UZ_utils.imread_gray(\".\\data\\\\newyork\\\\newyork_b.jpg\")\n",
    "correspondences = np.loadtxt(\".\\data\\\\newyork\\\\newyork.txt\")\n",
    "# print(correspondences)\n",
    "\n",
    "H = estimate_homography(correspondences)\n",
    "\n",
    "pts1 = np.column_stack((correspondences[:,0], correspondences[:,1]))\n",
    "# print(pts1)\n",
    "pts2 = np.column_stack((correspondences[:,2], correspondences[:,3]))\n",
    "# print(pts2)\n",
    "\n",
    "a4u.display_matches(I_a, pts1, I_b, pts2)\n",
    "\n",
    "I_a_warped = cv2.warpPerspective(I_a, H, I_a.shape[::-1])\n",
    "UZ_utils.imshow(I_a_warped)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "I_a = UZ_utils.imread_gray(\".\\data\\graf\\graf_a.jpg\")\n",
    "I_b = UZ_utils.imread_gray(\".\\data\\graf\\graf_b.jpg\")\n",
    "correspondences = np.loadtxt(\".\\data\\graf\\graf.txt\")\n",
    "# print(correspondences)\n",
    "\n",
    "H = estimate_homography(correspondences)\n",
    "\n",
    "pts1 = np.column_stack((correspondences[:,0], correspondences[:,1]))\n",
    "# print(pts1)\n",
    "pts2 = np.column_stack((correspondences[:,2], correspondences[:,3]))\n",
    "# print(pts2)\n",
    "\n",
    "a4u.display_matches(I_a, pts1, I_b, pts2)\n",
    "\n",
    "I_a_warped = cv2.warpPerspective(I_a, H, I_a.shape[::-1])\n",
    "\n",
    "UZ_utils.imshow(I_a_warped)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does find_points work on new york?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graf_a = UZ_utils.imread_gray(\".\\data\\\\newyork\\\\newyork_a.jpg\")\n",
    "graf_b = UZ_utils.imread_gray(\".\\data\\\\newyork\\\\newyork_b.jpg\")\n",
    "\n",
    "matched_pairs_of_ixs = find_matches_hessian(graf_a, graf_b)\n",
    "\n",
    "graf_a_array_for_display = np.zeros((len(matched_pairs_of_ixs), 2))\n",
    "graf_b_array_for_display = np.zeros((len(matched_pairs_of_ixs), 2))\n",
    "\n",
    "for i in range(len(matched_pairs_of_ixs)):\n",
    "    graf_a_array_for_display[i, 0] = matched_pairs_of_ixs[i][0][1]\n",
    "    graf_a_array_for_display[i, 1] = matched_pairs_of_ixs[i][0][0]\n",
    "    graf_b_array_for_display[i, 0] = matched_pairs_of_ixs[i][1][1]\n",
    "    graf_b_array_for_display[i, 1] = matched_pairs_of_ixs[i][1][0]\n",
    "\n",
    "\n",
    "a4u.display_matches(graf_a, graf_a_array_for_display, graf_b, graf_b_array_for_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: How many iterations on average did you need to find a good solution?\n",
    "How does the parameter choice for both the keypoint detector and RANSAC itself influence the performance (both quality and speed)?\n",
    "\n",
    "I had to set the treshold for the keypoint detector somewhat high. This meant only the best point pairs were detected.\n",
    "This way I could find a good solution in just 30 iterations of ransac, since the percentage of inliers was so high.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def reprojection_error(H, coresps):\n",
    "\n",
    "    # pts_a_x = coresps[:, 0]\n",
    "    # pts_a_y = coresps[:, 1]\n",
    "    # pts_b_x = coresps[:, 2]\n",
    "    # pts_b_y = coresps[:, 3]\n",
    "\n",
    "    pts_a_x = np.array([matched_points[i][0][1] for i in range(len(matched_points))])\n",
    "    pts_a_y = np.array([matched_points[i][0][0] for i in range(len(matched_points))])\n",
    "    pts_b_x = np.array([matched_points[i][1][1] for i in range(len(matched_points))])\n",
    "    pts_b_y = np.array([matched_points[i][1][0] for i in range(len(matched_points))])\n",
    "\n",
    "    distance = 0\n",
    "\n",
    "    for i in range(len(pts_a_x)):\n",
    "        pt_a = np.array([pts_a_x[i], pts_a_y[i], 1]).reshape((3,1))\n",
    "        new_pt = np.matmul(H, pt_a)\n",
    "        new_pt = new_pt[:2] / new_pt[2]\n",
    "\n",
    "        pt_b = np.array([pts_b_x[i], pts_b_y[i]]).reshape((2,1))\n",
    "        diff_vec = pt_b - new_pt\n",
    "        diff = np.sqrt(np.sum(diff_vec ** 2))\n",
    "\n",
    "        distance += diff\n",
    "    \n",
    "    return (distance / len(pts_a_x))\n",
    "\n",
    "def ransac(matched_points, num_of_iters):\n",
    "    # print(matched_points)\n",
    "    pts_a_x = np.array([matched_points[i][0][1] for i in range(len(matched_points))]).reshape(-1, 1)\n",
    "    pts_a_y = np.array([matched_points[i][0][0] for i in range(len(matched_points))]).reshape(-1, 1)\n",
    "    pts_b_x = np.array([matched_points[i][1][1] for i in range(len(matched_points))]).reshape(-1, 1)\n",
    "    pts_b_y = np.array([matched_points[i][1][0] for i in range(len(matched_points))]).reshape(-1, 1)\n",
    "\n",
    "    smallest_err = float('inf')\n",
    "    best_H = []\n",
    "    best_corresps = []\n",
    "\n",
    "\n",
    "    for i in range(num_of_iters):\n",
    "\n",
    "        chosen_four = random.sample(range(len(matched_points)), k=4)\n",
    "\n",
    "        # print(pts_a_x)\n",
    "        # print(chosen_four)\n",
    "        # print(pts_a_x[chosen_four])\n",
    "        \n",
    "        corresps = np.column_stack((pts_a_x[chosen_four], pts_a_y[chosen_four], pts_b_x[chosen_four], pts_b_y[chosen_four]))\n",
    "        # print(corresps)\n",
    "\n",
    "        H = estimate_homography(corresps)\n",
    "        # print(H)\n",
    "\n",
    "        rep_err = reprojection_error(H, matched_points)\n",
    "\n",
    "        if rep_err < smallest_err:\n",
    "            smallest_err = rep_err\n",
    "            best_H = H\n",
    "            best_corresps = corresps\n",
    "    \n",
    "    return best_corresps, best_H\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "I_a = UZ_utils.imread_gray(\".\\data\\\\newyork\\\\newyork_a.jpg\")\n",
    "I_b = UZ_utils.imread_gray(\".\\data\\\\newyork\\\\newyork_b.jpg\")\n",
    "\n",
    "\n",
    "matched_points = find_matches_hessian(I_a, I_b)\n",
    "    \n",
    "coresps, H = ransac(matched_points, num_of_iters=30)\n",
    "\n",
    "\n",
    "pts1 = np.column_stack((coresps[:,0], coresps[:,1]))\n",
    "# print(pts1)\n",
    "pts2 = np.column_stack((coresps[:,2], coresps[:,3]))\n",
    "# print(pts2)\n",
    "\n",
    "a4u.display_matches(I_a, pts1, I_b, pts2)\n",
    "\n",
    "I_a_warped = cv2.warpPerspective(I_a, H, I_a.shape[::-1])\n",
    "UZ_utils.imshow(I_a_warped)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# I_a = UZ_utils.imread_gray(\".\\data\\graf\\graf_a.jpg\")\n",
    "# I_b = UZ_utils.imread_gray(\".\\data\\graf\\graf_b.jpg\")\n",
    "\n",
    "\n",
    "# matched_points = find_matches_hessian(I_a, I_b)\n",
    "# print(matched_points)\n",
    "    \n",
    "# coresps, H = ransac(matched_points, num_of_iters=2000)\n",
    "\n",
    "\n",
    "# pts1 = np.column_stack((coresps[:,0], coresps[:,1]))\n",
    "# # print(pts1)\n",
    "# pts2 = np.column_stack((coresps[:,2], coresps[:,3]))\n",
    "# # print(pts2)\n",
    "\n",
    "# a4u.display_matches(I_a, pts1, I_b, pts2)\n",
    "\n",
    "# I_a_warped = cv2.warpPerspective(I_a, H, I_a.shape[::-1])\n",
    "# UZ_utils.imshow(I_a_warped)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_matches_hessian_parameter_set(image_a, image_b, sigma=3, t=0.004, non_max_neigh=7, n_bins=16):\n",
    "    \n",
    "\n",
    "    fp_image_ixs_a, simple_descs_a = simple_desc_hessian(image_a, sigma, t, non_max_neigh, n_bins)\n",
    "    fp_image_ixs_b, simple_descs_b = simple_desc_hessian(image_b, sigma, t, non_max_neigh, n_bins)\n",
    "\n",
    "    fp_image_ixs_a = list(zip(fp_image_ixs_a[0], fp_image_ixs_a[1]))\n",
    "    fp_image_ixs_b = list(zip(fp_image_ixs_b[0], fp_image_ixs_b[1]))\n",
    "\n",
    "    # these are the indexes of the other best matches, They point to their most corresponding point.\n",
    "    fp_a_best_matches, fp_b_best_matches = find_correspondances(simple_descs_a, simple_descs_b)\n",
    "\n",
    "    fp_symetric_matches = []\n",
    "\n",
    "    for a_ix in range(fp_a_best_matches.size):\n",
    "        matched_b_ix = fp_a_best_matches[a_ix]\n",
    "        matches_match = fp_b_best_matches[matched_b_ix]\n",
    "\n",
    "        if a_ix == matches_match:\n",
    "            fp_symetric_matches.append((fp_image_ixs_a[a_ix], fp_image_ixs_b[matched_b_ix]))\n",
    "\n",
    "    return fp_symetric_matches\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "I_a = UZ_utils.imread_gray(\".\\data\\graf\\graf_a.jpg\")\n",
    "I_b = UZ_utils.imread_gray(\".\\data\\graf\\graf_b.jpg\")\n",
    "\n",
    "\n",
    "g_sigmas = [4]\n",
    "g_ts = [0.008]\n",
    "\n",
    "for i, sigma in enumerate(g_sigmas):\n",
    "    for j, t in enumerate(g_ts):\n",
    "\n",
    "\n",
    "        matched_points = find_matches_hessian_parameter_set(I_a, I_b, sigma, t, non_max_neigh=7, n_bins=16)\n",
    "\n",
    "        graf_a_array_for_display = np.zeros((len(matched_points), 2))\n",
    "        graf_b_array_for_display = np.zeros((len(matched_points), 2))\n",
    "\n",
    "        for i in range(len(matched_points)):\n",
    "            graf_a_array_for_display[i, 0] = matched_points[i][0][1]\n",
    "            graf_a_array_for_display[i, 1] = matched_points[i][0][0]\n",
    "            graf_b_array_for_display[i, 0] = matched_points[i][1][1]\n",
    "            graf_b_array_for_display[i, 1] = matched_points[i][1][0]\n",
    "\n",
    "\n",
    "        a4u.display_matches(I_a, graf_a_array_for_display, I_b, graf_b_array_for_display)\n",
    "        # print(matched_points)\n",
    "            \n",
    "        coresps, H = ransac(matched_points, num_of_iters=30)\n",
    "\n",
    "\n",
    "        pts1 = np.column_stack((coresps[:,0], coresps[:,1]))\n",
    "        # print(pts1)\n",
    "        pts2 = np.column_stack((coresps[:,2], coresps[:,3]))\n",
    "        # print(pts2)\n",
    "\n",
    "        # plt.title(str(sigma) + \", tresh:\" + str(t))\n",
    "        a4u.display_matches(I_a, pts1, I_b, pts2)\n",
    "\n",
    "        I_a_warped = cv2.warpPerspective(I_a, H, I_a.shape[::-1])\n",
    "        UZ_utils.imshow(I_a_warped)\n",
    "\n",
    "\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
